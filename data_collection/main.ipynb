{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The folder ./data/htmls/data should be built beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"userAgents\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSDN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract html links for news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.get(\"https://www.cnbc.com/world/\") as response:\n",
    "    with open(\"data/htmls/links.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_set = set()\n",
    "with open(\"data/htmls/links.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read().rstrip()\n",
    "    matchObj = re.compile(r'<a href=\"(?P<title>.*?)\"', re.S)\n",
    "    with open(\"data/htmls/ref.txt\", \"w\", encoding=\"utf-8\") as wf:\n",
    "        for it in matchObj.finditer(data):\n",
    "            dic = it.groupdict()\n",
    "            if dic['title'] not in links_set:\n",
    "                links_set.add(dic['title'])\n",
    "                wf.write(f\"{dic['title']}\\n\" if \"https\" in dic[\"title\"] and \"html\" in dic[\"title\"] else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract html content of each html link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/htmls/ref.txt\", \"r\") as rf:\n",
    "    for idx, ref in enumerate(rf):\n",
    "        with requests.get(ref.strip(), headers=HEADERS) as response:\n",
    "            with open(f\"data/htmls/data/response{idx}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "        time.sleep(random.randint(20, 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract article content of each html content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from glob import glob\n",
    "with open(\"data/data.txt\", \"w\", encoding=\"utf-8\") as wf:\n",
    "    for i in trange(len(glob(\"data/htmls/data/*\"))):\n",
    "        with open(f\"data/htmls/data/response{i}.html\", 'r', encoding=\"utf-8\") as f:\n",
    "            webpage = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(webpage, \"html\")\n",
    "        wf.write(soup.title.text + '\\n')\n",
    "        for g in soup.find_all(\"div\", class_=\"group\"):\n",
    "            for p in g.find_all(\"p\"):\n",
    "                wf.write(p.text + '\\n')\n",
    "            for li in g.find_all(\"li\"):\n",
    "                wf.write(li.text + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract API content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace ... with your own api key\n",
    "APIKEY = \"...\"\n",
    "# Replace ... with the company you want to know e.g. QUERY = \"apple\"\n",
    "QUERY = \"...\"\n",
    "\n",
    "print(f\"https://newsapi.org/v2/everything?q={QUERY}&apiKey={APIKEY}&language=en\")\n",
    "\n",
    "titleset = set()\n",
    "articleset = set()\n",
    "\n",
    "for i in trange(1, 6):\n",
    "    with requests.get(f\"https://newsapi.org/v2/everything?q={QUERY}&apiKey={APIKEY}&language=en&page={i}\") as response:\n",
    "        resjson = response.json()\n",
    "        \n",
    "        for article in resjson[\"articles\"]:\n",
    "            title = article['title']\n",
    "            description = article.get('description', '')\n",
    "            titleset.add(title if title != \"[Removed]\" else \"\")\n",
    "            articleset.add(description if description != \"[Removed]\" else \"\")\n",
    "\n",
    "with open(\"data/data.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "    for title in titleset:\n",
    "        file.write(title + '\\n')\n",
    "    for descrpt in articleset:\n",
    "        if descrpt:\n",
    "            file.write(descrpt + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into meaningful sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "def lngdtct(text):\n",
    "    try:\n",
    "        lang=detect(text)\n",
    "    except:\n",
    "        return False\n",
    "    return lang == \"en\"\n",
    "\n",
    "lineset = set()\n",
    "\n",
    "def line_process(line: str) -> list:\n",
    "    return sent_tokenize(line)\n",
    "\n",
    "with open(\"data/data.txt\", \"r\", encoding=\"utf-8\") as readfile:\n",
    "    for line in readfile:\n",
    "        if lngdtct(line):\n",
    "            tmpline = re.sub(r\"\\.\\.\\.\", \"\", line)\n",
    "            tmpline = re.sub(r\"…\", \"\", tmpline)\n",
    "            lineset.add(tmpline)\n",
    "\n",
    "with open(\"data/processed_sentence.txt\", \"w\", encoding=\"utf-8\") as writefile:\n",
    "    for line in lineset:\n",
    "        for line in line_process(line):\n",
    "            if len(line) > 63:\n",
    "                writefile.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.api_core import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace ... with your own api key\n",
    "GOOGLE_API_KEY = \"...\"\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=5,\n",
    "    ))\n",
    "\n",
    "prompt = lambda str : f\"\"\"\n",
    "You are a financial expert.\n",
    "Classify the financial sentence as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Sentence: {str}\n",
    "\n",
    "EXAMPLE:\n",
    "Sentence: Investors prioritize Nvidia’s earnings over risk\n",
    "Response: POSITIVE\n",
    "\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "sentiment_list = []\n",
    "linelist = []\n",
    "removed_line = []\n",
    "\n",
    "with open(\"data/processed_sentence.txt\") as file:\n",
    "    for line in file:\n",
    "        linelist.append(line)\n",
    "\n",
    "for line in tqdm(linelist):\n",
    "    response = model.generate_content(prompt(line), request_options=retry_policy)\n",
    "    match = re.findall(r'\\b(POSITIVE|NEGATIVE|NEUTRAL)\\b', response.text)\n",
    "    if match:\n",
    "        sentiment_list.append(match[0])\n",
    "        time.sleep(random.randint(1, 2))\n",
    "    else:\n",
    "        removed_line.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/sentiment.txt\", \"w\") as file:\n",
    "    for sentiment in sentiment_list:\n",
    "        file.write(sentiment + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CSV file combining sentences and sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "sentence_list = []\n",
    "for line in linelist:\n",
    "    if line not in removed_line:\n",
    "        sentence_list.append(line)\n",
    "\n",
    "with open(\"data.csv\", \"w\", encoding=\"utf-8\") as csvfile:\n",
    "    csvwritter = csv.writer(csvfile)\n",
    "    csvwritter.writerow([\"Sentence\", \"Sentiment\"])\n",
    "    for sentence, sentiment in zip(sentence_list, sentiment_list):\n",
    "        csvwritter.writerow([sentence, sentiment.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove used files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "folder_path = \"./data\"\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(f\"Folder '{folder_path}' has been removed.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Folder '{folder_path}' does not exist.\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied while trying to delete '{folder_path}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
